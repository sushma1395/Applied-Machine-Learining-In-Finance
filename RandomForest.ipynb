{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import neighbors\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.optimize import minimize\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-04</td>\n",
       "      <td>32.290001</td>\n",
       "      <td>32.369999</td>\n",
       "      <td>32.580002</td>\n",
       "      <td>32.040001</td>\n",
       "      <td>8951900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-05</td>\n",
       "      <td>32.98</td>\n",
       "      <td>32.889999</td>\n",
       "      <td>33.439999</td>\n",
       "      <td>32.5</td>\n",
       "      <td>12256800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-06</td>\n",
       "      <td>32.349998</td>\n",
       "      <td>31.530001</td>\n",
       "      <td>32.5</td>\n",
       "      <td>31.16</td>\n",
       "      <td>11233600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-07</td>\n",
       "      <td>30.74</td>\n",
       "      <td>30.280001</td>\n",
       "      <td>30.950001</td>\n",
       "      <td>29.879999</td>\n",
       "      <td>16132600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-08</td>\n",
       "      <td>30.67</td>\n",
       "      <td>29.629999</td>\n",
       "      <td>30.700001</td>\n",
       "      <td>29.57</td>\n",
       "      <td>9961800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date       Open      Close       High        Low    Volume\n",
       "0  2016-01-04  32.290001  32.369999  32.580002  32.040001   8951900\n",
       "1  2016-01-05      32.98  32.889999  33.439999       32.5  12256800\n",
       "2  2016-01-06  32.349998  31.530001       32.5      31.16  11233600\n",
       "3  2016-01-07      30.74  30.280001  30.950001  29.879999  16132600\n",
       "4  2016-01-08      30.67  29.629999  30.700001      29.57   9961800"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"NVDA.csv\",decimal =',') \n",
    "df['Date']= pd.to_datetime(df['Date']) \n",
    "df['Date']=df['Date'].dt.date\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 879 entries, 0 to 878\n",
      "Data columns (total 6 columns):\n",
      "Date      878 non-null object\n",
      "Open      879 non-null object\n",
      "Close     879 non-null object\n",
      "High      879 non-null object\n",
      "Low       879 non-null object\n",
      "Volume    879 non-null object\n",
      "dtypes: object(6)\n",
      "memory usage: 41.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Open']=df['Open'].astype(float)\n",
    "df['Close']=df['Close'].astype(float)\n",
    "df['High']=df['High'].astype(float)\n",
    "df['Low']=df['Low'].astype(float)\n",
    "df['Volume']=df['Volume'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineering\n",
    "df['OC']=df['Open']-df['Close']\n",
    "df['AvgOC']=(df['Open']+df['Close'])/2\n",
    "df['HL']=df['High']-df['Low']\n",
    "df['AvgHL']=(df['High']+df['Low'])/2\n",
    "df['LV']=np.log(df['Volume'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selected Dataset - Variables used for training\n",
    "\n",
    "X=df[['Close','HL','OC','LV']]\n",
    "\n",
    "#Target Variable\n",
    "\n",
    "Y1=df['Open'].as_matrix()\n",
    "Y1=Y1[0:877]\n",
    "\n",
    "#Target variable Y as next day Opening value\n",
    "\n",
    "Y=df['Open'].shift(-1).as_matrix()\n",
    "T=877\n",
    "\n",
    "X=X[0:877]\n",
    "X=np.column_stack([np.ones((T,1)),X])\n",
    "\n",
    "Y=Y[0:877]\n",
    "#Random Split into Train and test with pareto principle of 80:20\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(Xt,Yt):\n",
    "    invXX = np.linalg.inv(Xt.transpose()@Xt)\n",
    "    beta_hat = invXX@Xt.transpose()@Yt\n",
    "    y_hat = Xt@beta_hat\n",
    "    return y_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GARCH(Y1):\n",
    "    mu = param0[0]\n",
    "    omega = param0[1]\n",
    "    alpha = param0[2]\n",
    "    beta = param0[3]\n",
    "    T = Y1.shape[0]\n",
    "    GARCH_Dens = np.zeros(T)\n",
    "    sigma2 = np.zeros(T)\n",
    "    F = np.zeros(T)\n",
    "    v = np.zeros(T)\n",
    "    sigma2[0] = omega/(1-alpha)\n",
    "    \n",
    "    for t in range(1,T):\n",
    "        sigma2[t] = omega+alpha*((Y1[t-1]-mu)**2)+beta*(sigma2[t-1])\n",
    "        F[t] = Y1[t] - mu-np.sqrt(sigma2[t])*np.random.normal(0,1,1)\n",
    "        v[t] = sigma2[t];\n",
    "        GARCH_Dens[t] = (1/2)*np.log(2*np.pi)+(1/2)*np.log(v[t])+\\\n",
    "                     (1/2)*(F[t]/v[t]);\n",
    "        Likelihood = np.sum(GARCH_Dens[1:-1])\n",
    "    return Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GARCH_PROD(params,T,Y1):\n",
    "    mu = param0[0]\n",
    "    omega = param0[1]\n",
    "    alpha = param0[2]\n",
    "    beta = param0[3]\n",
    "    #Y = np.zeros(T)\n",
    "    sigma2 = np.zeros(T)\n",
    "    sigma2[0]= omega/(1-alpha)\n",
    "    for t in range(1,T):\n",
    "        sigma2[t] = omega+alpha*((Y1[t-1]-mu)**2)+beta*(sigma2[t-1])\n",
    "        Y1[t] = mu+np.sqrt(sigma2[t])*np.random.normal(0,1,1)\n",
    "    return Y1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GARCH_PROD_t(params, Y0, T):\n",
    "    mu = params[0]\n",
    "    omega = np.exp(params[1])\n",
    "    alpha = 1/(1+np.exp(-params[2]))\n",
    "    beta = params[3]\n",
    "    nv = params[4]\n",
    "    #Y = np.zeros(T)\n",
    "    Y = (df['Open ']+df['Close'])/2\n",
    "    #Y = (df['Volume'])\n",
    "    sigma2 = np.zeros(T)\n",
    "    Y[0] = Y0\n",
    "    sigma2[0]= omega/(1-alpha)\n",
    "    for t in range(1,T):\n",
    "        sigma2[t] = omega+alpha*((Y[t-1]-mu)**2)+beta*(sigma2[t-1]);\n",
    "        Y[t] = mu+np.sqrt(sigma2[t])*np.random.normal(nv,1)\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kalman_Filter(YK):\n",
    "\n",
    "    S=YK.shape[0]\n",
    "    z = 1\n",
    "    Z = 1\n",
    "    T = 0.5\n",
    "    H = np.var(YK);\n",
    "    Q = 0.5*np.var(YK)\n",
    "    #Kalman Filter Starts\n",
    "    u_predict = np.zeros(S)\n",
    "    u_update = np.zeros(S)\n",
    "    p_predict = np.zeros(S)\n",
    "    p_update = np.zeros(S)\n",
    "    v = np.zeros(S)\n",
    "    F = np.zeros(S)\n",
    "    KF_Dens = np.zeros(S)\n",
    "\n",
    "    for s in range(1,S):\n",
    "        if s ==1:\n",
    "            p_update[s] =1000\n",
    "            p_predict[s] = T*p_update[1]*np.transpose(T)+Q\n",
    "        else:\n",
    "            F[s] = z*p_predict[s-1]*np.transpose(z)+H\n",
    "            v[s] = YK[s-1]-z*u_predict[s-1]\n",
    "\n",
    "            u_update[s] =T*u_update[s-1]+p_predict[s-1]*np.transpose(Z)*(1/F[s])*v[s]\n",
    "            u_predict[s] = T*u_update[s]\n",
    "\n",
    "            p_update[s] = p_predict[s-1]-p_predict[s-1]*np.transpose(Z)*(1/F[s])*Z*p_predict[s-1]\n",
    "            p_predict[s] = T*p_update[s]*np.transpose(T)+Q\n",
    "            KF_Dens[s]=(1/2)*np.log(2*np.pi)+(1/2)*np.log(abs(F[s]))+(1/2)*np.transpose(v[s])*(1/F[s])\n",
    "            Likelihood=sum(KF_Dens[1:-1])\n",
    "    return Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kalman_Smoother(params,YKS):\n",
    "    S=YKS.shape[0]\n",
    "    Z = params[0]\n",
    "    T = params[1]\n",
    "    H = params[2]\n",
    "    Q = params[3]\n",
    "    #Kalman Filter Starts\n",
    "    u_predict = np.zeros(S)\n",
    "    u_update = np.zeros(S)\n",
    "    p_predict = np.zeros(S)\n",
    "    p_update = np.zeros(S)\n",
    "    v = np.zeros(S)\n",
    "    F = np.zeros(S)\n",
    "    KF_Dens = np.zeros(S)\n",
    "\n",
    "    for s in range(1,S):\n",
    "        if s ==1:\n",
    "            p_update[s] =1000\n",
    "            p_predict[s] = T*p_update[1]*np.transpose(T)+Q\n",
    "        else:\n",
    "            F[s] = Z*p_predict[s-1]*np.transpose(Z)+H\n",
    "            v[s] = YKS[s-1]-Z*u_predict[s-1]\n",
    "            \n",
    "            u_update[s] =T*u_update[s-1]+p_predict[s-1]*np.transpose(Z)*(1/F[s])*v[s]\n",
    "            u_predict[s] = T*u_update[s]\n",
    "\n",
    "            p_update[s] = p_predict[s-1]-p_predict[s-1]*np.transpose(Z)*(1/F[s])*Z*p_predict[s-1]\n",
    "            p_predict[s] = T*p_update[s]*np.transpose(T)+Q\n",
    "        \n",
    "    u_smooth = np.zeros(S)\n",
    "    p_smooth = np.zeros(S)\n",
    "    u_smooth[S-1] = u_update[S-1]\n",
    "    p_smooth[S-1] = p_update[S-1]\n",
    "    for t in range (S-1,0,-1):\n",
    "        u_smooth[t-1] =u_update[t]+p_update[t]*np.transpose(T)/p_predict[t]*(u_smooth[t]-T*u_update[t])\n",
    "        p_smooth[t-1] = p_update[t]+p_update[t]*np.transpose(T)/p_predict[t]*(p_smooth[t]-p_predict[t])/p_predict[t]\n",
    "    return u_smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MovingAvg(x, y):\n",
    "    preds = []\n",
    "    for i in range(0,y.shape[0]):\n",
    "        a = df['Open'][len(df)-301+i:].sum() + sum(preds)\n",
    "        b = a/y.shape[0]\n",
    "        preds.append(b)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y1Temp=Y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "val=MovingAvg(X,Y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.904860\n",
      "         Iterations: 1\n",
      "         Function evaluations: 114\n",
      "         Gradient evaluations: 17\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.822935\n",
      "         Iterations: 4\n",
      "         Function evaluations: 239\n",
      "         Gradient evaluations: 37\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.989127\n",
      "         Iterations: 3\n",
      "         Function evaluations: 206\n",
      "         Gradient evaluations: 31\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.937998\n",
      "         Iterations: 1\n",
      "         Function evaluations: 131\n",
      "         Gradient evaluations: 20\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Divide-by-zero encountered: rhok assumed large\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 5.037511\n",
      "         Iterations: 2\n",
      "         Function evaluations: 192\n",
      "         Gradient evaluations: 30\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.832884\n",
      "         Iterations: 3\n",
      "         Function evaluations: 138\n",
      "         Gradient evaluations: 21\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.982273\n",
      "         Iterations: 2\n",
      "         Function evaluations: 203\n",
      "         Gradient evaluations: 31\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.996998\n",
      "         Iterations: 1\n",
      "         Function evaluations: 138\n",
      "         Gradient evaluations: 21\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 5.002180\n",
      "         Iterations: 1\n",
      "         Function evaluations: 156\n",
      "         Gradient evaluations: 24\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.963586\n",
      "         Iterations: 4\n",
      "         Function evaluations: 193\n",
      "         Gradient evaluations: 30\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.966955\n",
      "         Iterations: 1\n",
      "         Function evaluations: 120\n",
      "         Gradient evaluations: 18\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Divide-by-zero encountered: rhok assumed large\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 5.048944\n",
      "         Iterations: 2\n",
      "         Function evaluations: 172\n",
      "         Gradient evaluations: 27\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.940284\n",
      "         Iterations: 1\n",
      "         Function evaluations: 120\n",
      "         Gradient evaluations: 18\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.991092\n",
      "         Iterations: 1\n",
      "         Function evaluations: 167\n",
      "         Gradient evaluations: 26\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 5.083046\n",
      "         Iterations: 2\n",
      "         Function evaluations: 171\n",
      "         Gradient evaluations: 27\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.928330\n",
      "         Iterations: 2\n",
      "         Function evaluations: 198\n",
      "         Gradient evaluations: 30\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.947021\n",
      "         Iterations: 3\n",
      "         Function evaluations: 192\n",
      "         Gradient evaluations: 29\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.991955\n",
      "         Iterations: 2\n",
      "         Function evaluations: 150\n",
      "         Gradient evaluations: 23\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Divide-by-zero encountered: rhok assumed large\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.905056\n",
      "         Iterations: 5\n",
      "         Function evaluations: 334\n",
      "         Gradient evaluations: 52\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.935478\n",
      "         Iterations: 1\n",
      "         Function evaluations: 144\n",
      "         Gradient evaluations: 22\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.940344\n",
      "         Iterations: 3\n",
      "         Function evaluations: 201\n",
      "         Gradient evaluations: 31\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 5.027315\n",
      "         Iterations: 1\n",
      "         Function evaluations: 167\n",
      "         Gradient evaluations: 26\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.941774\n",
      "         Iterations: 2\n",
      "         Function evaluations: 198\n",
      "         Gradient evaluations: 31\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.935259\n",
      "         Iterations: 1\n",
      "         Function evaluations: 138\n",
      "         Gradient evaluations: 21\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.930585\n",
      "         Iterations: 1\n",
      "         Function evaluations: 144\n",
      "         Gradient evaluations: 22\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.885626\n",
      "         Iterations: 2\n",
      "         Function evaluations: 186\n",
      "         Gradient evaluations: 29\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Divide-by-zero encountered: rhok assumed large\n",
      "Divide-by-zero encountered: rhok assumed large\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.951843\n",
      "         Iterations: 4\n",
      "         Function evaluations: 246\n",
      "         Gradient evaluations: 38\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.971334\n",
      "         Iterations: 3\n",
      "         Function evaluations: 144\n",
      "         Gradient evaluations: 22\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.983675\n",
      "         Iterations: 2\n",
      "         Function evaluations: 193\n",
      "         Gradient evaluations: 29\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.998498\n",
      "         Iterations: 3\n",
      "         Function evaluations: 199\n",
      "         Gradient evaluations: 31\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.812805\n",
      "         Iterations: 2\n",
      "         Function evaluations: 187\n",
      "         Gradient evaluations: 29\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.986362\n",
      "         Iterations: 2\n",
      "         Function evaluations: 149\n",
      "         Gradient evaluations: 23\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Divide-by-zero encountered: rhok assumed large\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 5.033010\n",
      "         Iterations: 3\n",
      "         Function evaluations: 174\n",
      "         Gradient evaluations: 27\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 5.064975\n",
      "         Iterations: 1\n",
      "         Function evaluations: 149\n",
      "         Gradient evaluations: 23\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 5.012119\n",
      "         Iterations: 1\n",
      "         Function evaluations: 185\n",
      "         Gradient evaluations: 29\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.971799\n",
      "         Iterations: 1\n",
      "         Function evaluations: 174\n",
      "         Gradient evaluations: 27\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.877198\n",
      "         Iterations: 3\n",
      "         Function evaluations: 221\n",
      "         Gradient evaluations: 33\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.997310\n",
      "         Iterations: 1\n",
      "         Function evaluations: 167\n",
      "         Gradient evaluations: 26\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.901411\n",
      "         Iterations: 0\n",
      "         Function evaluations: 156\n",
      "         Gradient evaluations: 24\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.996319\n",
      "         Iterations: 0\n",
      "         Function evaluations: 162\n",
      "         Gradient evaluations: 25\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 5.046517\n",
      "         Iterations: 1\n",
      "         Function evaluations: 137\n",
      "         Gradient evaluations: 21\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.946332\n",
      "         Iterations: 1\n",
      "         Function evaluations: 126\n",
      "         Gradient evaluations: 19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 5.012589\n",
      "         Iterations: 1\n",
      "         Function evaluations: 173\n",
      "         Gradient evaluations: 27\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Divide-by-zero encountered: rhok assumed large\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 5.043458\n",
      "         Iterations: 2\n",
      "         Function evaluations: 174\n",
      "         Gradient evaluations: 27\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.887684\n",
      "         Iterations: 2\n",
      "         Function evaluations: 150\n",
      "         Gradient evaluations: 23\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.923475\n",
      "         Iterations: 4\n",
      "         Function evaluations: 254\n",
      "         Gradient evaluations: 40\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.922704\n",
      "         Iterations: 2\n",
      "         Function evaluations: 182\n",
      "         Gradient evaluations: 28\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.900995\n",
      "         Iterations: 2\n",
      "         Function evaluations: 238\n",
      "         Gradient evaluations: 36\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.936488\n",
      "         Iterations: 2\n",
      "         Function evaluations: 167\n",
      "         Gradient evaluations: 26\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 5.081561\n",
      "         Iterations: 1\n",
      "         Function evaluations: 161\n",
      "         Gradient evaluations: 25\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.929744\n",
      "         Iterations: 1\n",
      "         Function evaluations: 150\n",
      "         Gradient evaluations: 23\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.921917\n",
      "         Iterations: 0\n",
      "         Function evaluations: 156\n",
      "         Gradient evaluations: 24\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.966963\n",
      "         Iterations: 1\n",
      "         Function evaluations: 126\n",
      "         Gradient evaluations: 19\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.887327\n",
      "         Iterations: 2\n",
      "         Function evaluations: 237\n",
      "         Gradient evaluations: 36\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.879664\n",
      "         Iterations: 2\n",
      "         Function evaluations: 190\n",
      "         Gradient evaluations: 28\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 5.047278\n",
      "         Iterations: 1\n",
      "         Function evaluations: 137\n",
      "         Gradient evaluations: 21\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.887343\n",
      "         Iterations: 1\n",
      "         Function evaluations: 126\n",
      "         Gradient evaluations: 19\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Divide-by-zero encountered: rhok assumed large\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.897046\n",
      "         Iterations: 1\n",
      "         Function evaluations: 180\n",
      "         Gradient evaluations: 28\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.930596\n",
      "         Iterations: 2\n",
      "         Function evaluations: 144\n",
      "         Gradient evaluations: 22\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.868558\n",
      "         Iterations: 2\n",
      "         Function evaluations: 201\n",
      "         Gradient evaluations: 31\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.959295\n",
      "         Iterations: 3\n",
      "         Function evaluations: 156\n",
      "         Gradient evaluations: 24\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.910057\n",
      "         Iterations: 3\n",
      "         Function evaluations: 207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Gradient evaluations: 32\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.907183\n",
      "         Iterations: 2\n",
      "         Function evaluations: 132\n",
      "         Gradient evaluations: 20\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.922014\n",
      "         Iterations: 1\n",
      "         Function evaluations: 131\n",
      "         Gradient evaluations: 20\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.967254\n",
      "         Iterations: 1\n",
      "         Function evaluations: 144\n",
      "         Gradient evaluations: 22\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.941448\n",
      "         Iterations: 1\n",
      "         Function evaluations: 126\n",
      "         Gradient evaluations: 19\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Divide-by-zero encountered: rhok assumed large\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 5.001497\n",
      "         Iterations: 1\n",
      "         Function evaluations: 198\n",
      "         Gradient evaluations: 31\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.947511\n",
      "         Iterations: 1\n",
      "         Function evaluations: 150\n",
      "         Gradient evaluations: 23\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Divide-by-zero encountered: rhok assumed large\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.923857\n",
      "         Iterations: 3\n",
      "         Function evaluations: 198\n",
      "         Gradient evaluations: 30\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.899861\n",
      "         Iterations: 3\n",
      "         Function evaluations: 156\n",
      "         Gradient evaluations: 24\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.975116\n",
      "         Iterations: 1\n",
      "         Function evaluations: 168\n",
      "         Gradient evaluations: 26\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.957122\n",
      "         Iterations: 0\n",
      "         Function evaluations: 156\n",
      "         Gradient evaluations: 24\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.949885\n",
      "         Iterations: 2\n",
      "         Function evaluations: 131\n",
      "         Gradient evaluations: 20\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.966049\n",
      "         Iterations: 2\n",
      "         Function evaluations: 174\n",
      "         Gradient evaluations: 27\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.897127\n",
      "         Iterations: 2\n",
      "         Function evaluations: 138\n",
      "         Gradient evaluations: 21\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 5.037253\n",
      "         Iterations: 1\n",
      "         Function evaluations: 178\n",
      "         Gradient evaluations: 28\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 5.046584\n",
      "         Iterations: 1\n",
      "         Function evaluations: 144\n",
      "         Gradient evaluations: 22\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Divide-by-zero encountered: rhok assumed large\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.917252\n",
      "         Iterations: 4\n",
      "         Function evaluations: 228\n",
      "         Gradient evaluations: 36\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.880969\n",
      "         Iterations: 2\n",
      "         Function evaluations: 185\n",
      "         Gradient evaluations: 28\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.907710\n",
      "         Iterations: 2\n",
      "         Function evaluations: 180\n",
      "         Gradient evaluations: 27\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.996594\n",
      "         Iterations: 1\n",
      "         Function evaluations: 143\n",
      "         Gradient evaluations: 22\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 5.054985\n",
      "         Iterations: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Function evaluations: 156\n",
      "         Gradient evaluations: 24\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.855746\n",
      "         Iterations: 3\n",
      "         Function evaluations: 181\n",
      "         Gradient evaluations: 28\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Divide-by-zero encountered: rhok assumed large\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.960071\n",
      "         Iterations: 3\n",
      "         Function evaluations: 197\n",
      "         Gradient evaluations: 31\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.975969\n",
      "         Iterations: 4\n",
      "         Function evaluations: 200\n",
      "         Gradient evaluations: 31\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.930797\n",
      "         Iterations: 1\n",
      "         Function evaluations: 132\n",
      "         Gradient evaluations: 20\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.960950\n",
      "         Iterations: 1\n",
      "         Function evaluations: 150\n",
      "         Gradient evaluations: 23\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.891299\n",
      "         Iterations: 1\n",
      "         Function evaluations: 120\n",
      "         Gradient evaluations: 18\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.993476\n",
      "         Iterations: 1\n",
      "         Function evaluations: 107\n",
      "         Gradient evaluations: 16\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.897765\n",
      "         Iterations: 3\n",
      "         Function evaluations: 242\n",
      "         Gradient evaluations: 37\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.960928\n",
      "         Iterations: 2\n",
      "         Function evaluations: 131\n",
      "         Gradient evaluations: 20\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 5.076013\n",
      "         Iterations: 3\n",
      "         Function evaluations: 186\n",
      "         Gradient evaluations: 29\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.899630\n",
      "         Iterations: 1\n",
      "         Function evaluations: 198\n",
      "         Gradient evaluations: 31\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 5.015468\n",
      "         Iterations: 3\n",
      "         Function evaluations: 167\n",
      "         Gradient evaluations: 26\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.970286\n",
      "         Iterations: 2\n",
      "         Function evaluations: 149\n",
      "         Gradient evaluations: 23\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.946646\n",
      "         Iterations: 2\n",
      "         Function evaluations: 188\n",
      "         Gradient evaluations: 29\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.988749\n",
      "         Iterations: 4\n",
      "         Function evaluations: 242\n",
      "         Gradient evaluations: 37\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.901017\n",
      "         Iterations: 0\n",
      "         Function evaluations: 150\n",
      "         Gradient evaluations: 23\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 4.944995\n",
      "         Iterations: 3\n",
      "         Function evaluations: 204\n",
      "         Gradient evaluations: 31\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 5.026775\n",
      "         Iterations: 0\n",
      "         Function evaluations: 156\n",
      "         Gradient evaluations: 24\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -11109.886400\n",
      "         Iterations: 53\n",
      "         Function evaluations: 432\n",
      "         Gradient evaluations: 70\n"
     ]
    }
   ],
   "source": [
    "df_yhatLinearTrain=pd.DataFrame()\n",
    "df_yhatGarchTrain=pd.DataFrame()\n",
    "df_yhatGarchTTrain=pd.DataFrame()\n",
    "df_yhatKalman=pd.DataFrame()\n",
    "df_yhatMA=pd.DataFrame()\n",
    "\n",
    "df_yhatLinearTest=pd.DataFrame()\n",
    "df_yhatGarchTest=pd.DataFrame()\n",
    "df_yhatGarchTTest=pd.DataFrame()\n",
    "Y1Temp=Y1\n",
    "\n",
    "for x in range(0,100):\n",
    "    #Linear for Train dataset\n",
    "    df_yhatLinearTrain[x]=linear(X_train,y_train)\n",
    "    \n",
    "#     \"\"\"Linear for Test dataset\"\"\"\n",
    "#     df_yhatLinearTest[x]=linear(X_test,y_test)\n",
    "\n",
    "    #Garch for Train dataset\n",
    "    Y1=Y1Temp\n",
    "    T = len(Y1)\n",
    "    param0 = np.array([32,35,0.1,0.1])\n",
    "    param_star = minimize(GARCH, param0, method ='BFGS', options ={'xtol':1e-8, 'disp':True})\n",
    "    Y_GARCH = GARCH_PROD(param_star.x,T,Y1)\n",
    "    df_yhatGarchTrain[x]=Y_GARCH\n",
    "    \n",
    "    Y1=Y1Temp\n",
    "    param0 = np.array([1.3,0.3,8,9])\n",
    "    param_star = minimize(Kalman_Filter, param0, method = 'BFGS', options ={'xtol':1e-8,'disp': True})\n",
    "    Y_update = Kalman_Smoother(param_star.x,Y1)\n",
    "    df_yhatKalman[x]=Y_update\n",
    "    \n",
    "    Y1=Y1Temp\n",
    "    df_yhatMA[x]=MovingAvg(X,Y1)\n",
    "#     \"\"\"Garch for Test dataset\"\"\"\n",
    "#     T = len(y_test)\n",
    "#     mu = 35\n",
    "#     sig =5\n",
    "#     param0 = np.array([32,35,0.2,0.5])\n",
    "#     param_star = minimize(GARCH, param0, method ='BFGS', options ={'xtol':1e-8, 'disp':True})\n",
    "#     Y_GARCH = GARCH_PROD(param_star.x,T,y_test)\n",
    "#     df_yhatGarchTest[x]=Y_GARCH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "resdata={}\n",
    "y_hatTrainLinear=df_yhatLinearTrain.mean(axis=1)\n",
    "reslinT=y_train-y_hatTrainLinear\n",
    "\n",
    "y_hatTrainGarch=df_yhatGarchTrain.mean(axis=1)\n",
    "resG=Y1Temp-y_hatTrainGarch\n",
    "\n",
    "y_hatKalman=df_yhatKalman.mean(axis=1)\n",
    "resK=Y1Temp-y_hatKalman\n",
    "\n",
    "\n",
    "y_hatMA=df_yhatMA.mean(axis=1)\n",
    "resMA=Y1Temp-y_hatMA\n",
    "\n",
    "resdata={'Linear':reslinT,'Garch':resG,'Kalman':resK,'Moving Average':resMA}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomForest(datadict):\n",
    "    data={'Algo':'','RMSE':None}\n",
    "    for key,val in datadict.items():\n",
    "        rmse=np.sqrt(((val) ** 2).mean())\n",
    "        if(data['RMSE']==None):\n",
    "            data['Algo']=key\n",
    "            data['RMSE']=rmse\n",
    "        elif data['RMSE']>rmse:\n",
    "            data['Algo']=key\n",
    "            data['RMSE']=rmse\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8080678276157753"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "rmse=np.sqrt(((reslinT) ** 2).mean())\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranndomforestoutput=randomForest(resdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Algo': 'Linear', 'RMSE': 2.8080678276157753}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranndomforestoutput"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
